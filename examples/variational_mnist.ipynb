{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational inference (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../torchutils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% import\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchutils import ClassifierTraining\n",
    "from vartorch import \\\n",
    "    VariationalClassifier, \\\n",
    "    VariationalLinear, \\\n",
    "    accuracy_vs_confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% transformations\n",
    "preprocessor = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% datasets\n",
    "data_path = pathlib.Path.home() / 'Data'\n",
    "train_set = datasets.MNIST(data_path,\n",
    "                           train=True,\n",
    "                           transform=preprocessor,\n",
    "                           download=True)\n",
    "test_set = datasets.MNIST(data_path,\n",
    "                          train=False,\n",
    "                          transform=preprocessor,\n",
    "                          download=True)\n",
    "print('No. train images:', len(train_set))\n",
    "print('No. test images:', len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(test_set,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "print('No. train batches:', len(train_loader))\n",
    "print('No. test batches:', len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% example images\n",
    "data_iterator = iter(train_loader)\n",
    "images, labels = next(data_iterator)\n",
    "print('Images shape:', images.shape)\n",
    "print('Labels shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: example images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(5, 3))\n",
    "for idx, ax in enumerate(axes.ravel()):\n",
    "    image = images[idx,0].numpy()\n",
    "    ax.imshow(image.clip(0,1), cmap='gray')\n",
    "    ax.set_title(train_set.classes[labels[idx]])\n",
    "    ax.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% model (logistic regression)\n",
    "# model = nn.Sequential(\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(in_features=28*28, out_features=10),\n",
    "# )\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% model (small CNN with linear classifier)\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(5,5), padding=2),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=4, out_channels=8, kernel_size=(5,5), padding=2),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=7*7*8, out_features=10)\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% standard model\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "point_model = ClassifierTraining(model, criterion, optimizer, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% training\n",
    "point_history = point_model.training(no_epochs=10, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% testing\n",
    "point_train_loss, point_train_acc = point_model.test(train_loader)\n",
    "point_test_loss, point_test_acc = point_model.test(test_loader)\n",
    "print('Train loss: {:.4f}'.format(point_train_loss))\n",
    "print('Test loss: {:.4f}'.format(point_test_loss))\n",
    "print('Train acc.: {:.4f}'.format(point_train_acc))\n",
    "print('Test acc.: {:.4f}'.format(point_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: training history\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.array(point_history['train_loss']), label='training', alpha=0.7)\n",
    "ax.plot(np.array(point_history['test_loss']), label='testing', alpha=0.7)\n",
    "ax.set(xlabel='epoch', ylabel='loss')\n",
    "ax.set_xlim([0, point_history['no_epochs']])\n",
    "ax.legend()\n",
    "ax.grid(b=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% model (variational logistic regression)\n",
    "# model = nn.Sequential(\n",
    "#     nn.Flatten(),\n",
    "#     VariationalLinear(in_features=28*28, out_features=10),\n",
    "# )\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% model (small CNN with variational linear classifier)\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(5,5), padding=2),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=4, out_channels=8, kernel_size=(5,5), padding=2),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    VariationalLinear(in_features=7*7*8, out_features=10)\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% variational inference\n",
    "post_model = VariationalClassifier(model, likelihood_type='Categorical')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "post_model.compile_for_training(optimizer, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% training\n",
    "post_history = post_model.training(no_epochs=20, no_samples=10, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% testing\n",
    "post_train_loss = post_model.test_loss(train_loader)\n",
    "post_train_acc = post_model.test_acc(train_loader)\n",
    "post_test_loss = post_model.test_loss(test_loader)\n",
    "post_test_acc = post_model.test_acc(test_loader)\n",
    "print('Train loss: {:.4f}'.format(post_train_loss))\n",
    "print('Test loss: {:.4f}'.format(post_test_loss))\n",
    "print('Train acc.: {:.4f}'.format(post_train_acc))\n",
    "print('Test acc.: {:.4f}'.format(post_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: training history\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(-np.array(post_history['train_loss']), label='training', alpha=0.7)\n",
    "ax.plot(-np.array(post_history['test_loss']), label='testing', alpha=0.7)\n",
    "ax.set(xlabel='epoch', ylabel='ELBO')\n",
    "ax.set_xlim([0, post_history['no_epochs']])\n",
    "ax.legend()\n",
    "ax.grid(b=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% normal MNIST\n",
    "data_set = test_set\n",
    "data_loader = test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% another MNIST\n",
    "# data_set = datasets.KMNIST(data_path, train=False, transform=preprocessor, download=True)\n",
    "# # data_set = datasets.FashionMNIST(data_path, train=False, transform=preprocessor, download=True)\n",
    "# data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% random noise\n",
    "# data_set = TensorDataset(torch.rand(batch_size, 1, 28, 28), torch.zeros((batch_size,), dtype=torch.int64))\n",
    "# data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% example data\n",
    "data_iterator = iter(data_loader)\n",
    "images, labels = next(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% standard point predictions\n",
    "point_model.train(False)\n",
    "with torch.no_grad():\n",
    "    point_logits = point_model.predict(images)\n",
    "    point_probs = torch.softmax(point_logits, dim=1)\n",
    "    point_top_prob, point_top_class = torch.topk(point_probs, k=1, dim=1)\n",
    "    point_entropy = dist.Categorical(probs=point_probs).entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% posterior mean point predictions\n",
    "# post_model.sample(False)\n",
    "# post_model.train(False)\n",
    "# with torch.no_grad():\n",
    "#     point_logits = post_model.predict(images)\n",
    "#     point_probs = torch.softmax(point_logits, dim=1)\n",
    "#     point_top_prob, point_top_class = torch.topk(point_probs, k=1, dim=1)\n",
    "#     point_entropy = dist.Categorical(probs=point_probs).entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% posterior samples predictions\n",
    "no_samples = 500\n",
    "post_model.sample(True)\n",
    "post_model.train(False)\n",
    "with torch.no_grad():\n",
    "    sampled_logits = post_model.predict(images, no_samples)\n",
    "    sampled_probs = torch.softmax(sampled_logits, dim=1)\n",
    "    sampled_top_prob, sampled_top_class = torch.topk(sampled_probs, k=1, dim=1)\n",
    "    post_probs = torch.mean(sampled_probs, axis=-1)\n",
    "    post_top_prob, post_top_class = torch.topk(post_probs, k=1, dim=1)\n",
    "    is_consistent = sampled_top_class == post_top_class.unsqueeze(-1)\n",
    "    post_consistency = torch.mean(is_consistent.float(), dim=-1).squeeze()\n",
    "    post_entropy = dist.Categorical(probs=post_probs).entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot ids\n",
    "plot_ids = np.random.permutation(np.arange(len(images))) # random\n",
    "# plot_ids = torch.argsort(point_entropy, descending=False).data.numpy() # lowest entropy\n",
    "# plot_ids = torch.argsort(post_consistency, descending=False).data.numpy() # lowest consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: point pedictions\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(5,6))\n",
    "for idx, (ax1, ax2) in zip(plot_ids[:axes.shape[0]], axes):\n",
    "    # image\n",
    "    image = images[idx,0].numpy()\n",
    "    ax1.imshow(image.clip(0,1), cmap='gray')\n",
    "    ax1.set_title('{}'.format(data_set.classes[labels[idx]])\n",
    "                  if hasattr(data_set, 'classes') else 'random')\n",
    "    ax1.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "    # probabilities\n",
    "    ax2.bar(np.arange(10), point_probs.data.numpy()[idx])\n",
    "    ax2.set_title('$\\pi(c|x,\\hat{w})$')\n",
    "    ax2.set(xticks=np.arange(10), ylim=[0,1], xlabel='c')\n",
    "    # ax2.text(0, 0.75, 'entropy: {:.2f}'.format(point_entropy[idx]), alpha=0.5)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: posterior predictions\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(8,6))\n",
    "for idx, (ax1, ax3, ax4) in zip(plot_ids[:axes.shape[0]], axes):\n",
    "    # image\n",
    "    image = images[idx,0].numpy()\n",
    "    ax1.imshow(image.clip(0,1), cmap='gray')\n",
    "    ax1.set_title('{}'.format(data_set.classes[labels[idx]])\n",
    "                  if hasattr(data_set, 'classes') else 'random noise')\n",
    "    ax1.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "    # violin plot\n",
    "    # ax2.violinplot(sampled_probs[idx,:,:], positions=np.arange(10))\n",
    "    # ax2.set_title('$\\pi(c|x,w)$, $w$ from $\\pi(w|\\mathcal{D})$')\n",
    "    # ax2.set(xticks=np.arange(10), ylim=[0,1], xlabel='c')\n",
    "    # ax2.text(0, 0.75, 'consistency: {:.2f}'.format(post_consistency[idx]), alpha=0.5)\n",
    "    # histogram\n",
    "    highest_ids = post_probs[idx].data.numpy().argsort()[::-1][:3]\n",
    "    for highest_idx in highest_ids:\n",
    "        ax3.hist(sampled_probs[idx,highest_idx,:], bins=50, range=[0,1],\n",
    "                 density=True, histtype='stepfilled', alpha=0.5)\n",
    "    ax3.set_title('$\\pi(c|x,w)$, $w$ from $\\pi(w|\\mathcal{D})$')\n",
    "    ax3.set_xlim([0,1])\n",
    "    ax3.legend(['c={}'.format(c) for c in highest_ids], loc='upper center')\n",
    "    ax3.grid(b=True, which='both', color='lightgray', linestyle='-')\n",
    "    ax3.set_axisbelow(True)\n",
    "    # posterior predictive\n",
    "    ax4.bar(np.arange(10), post_probs[idx].data.numpy())\n",
    "    ax4.set_title('$\\pi(c|x,\\mathcal{D}) = \\int \\pi(c|x,w) \\pi(w|\\mathcal{D}) dw$')\n",
    "    ax4.set(xticks=np.arange(10), ylim=[0,1], xlabel='c')\n",
    "    # ax4.text(0, 0.75, 'entropy: {:.2f}'.format(post_entropy[idx]), alpha=0.5)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-distribution detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% normal MNIST\n",
    "norm_set = test_set\n",
    "norm_loader = test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% another MNIST\n",
    "anom_set = datasets.KMNIST(data_path, train=False, transform=preprocessor, download=True)\n",
    "# anom_set = datasets.FashionMNIST(data_path, train=False, transform=preprocessor, download=True)\n",
    "anom_loader = DataLoader(anom_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% function definitions\n",
    "def entropy_and_maxprob(predict_proba, data_loader):\n",
    "    '''Compute entropy and max. confidence.'''\n",
    "    probs_list = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            probs = predict_proba(images)\n",
    "            probs_list.append(probs)\n",
    "        probs = torch.cat([probs for probs in probs_list], dim=0)\n",
    "        entropy = dist.Categorical(probs=probs).entropy()\n",
    "        top_prob, top_class = torch.topk(probs, k=1, dim=1)\n",
    "    return entropy, top_prob\n",
    "point_predict_proba = lambda images: torch.softmax(point_model.predict(images), dim=1)\n",
    "post_predict_proba = lambda images: post_model.predict_proba(images, no_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% entropies for normal/anomalous data\n",
    "point_norm_entropy, point_norm_maxprob = entropy_and_maxprob(point_predict_proba, norm_loader)\n",
    "point_anom_entropy, point_anom_maxprob = entropy_and_maxprob(point_predict_proba, anom_loader)\n",
    "post_norm_entropy, post_norm_maxprob = entropy_and_maxprob(post_predict_proba, norm_loader)\n",
    "post_anom_entropy, post_anom_maxprob = entropy_and_maxprob(post_predict_proba, anom_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: point entropy histogram\n",
    "fig, ax = plt.subplots(figsize=(5,3.5))\n",
    "ax.hist(point_norm_entropy.data.numpy().squeeze(),\n",
    "        bins=100, range=(0,2), density=True, histtype='stepfilled',\n",
    "        alpha=0.7, label='in distribution')\n",
    "ax.hist(point_anom_entropy.data.numpy().squeeze(),\n",
    "        bins=100, range=(0,2), density=True, histtype='stepfilled',\n",
    "        alpha=0.7, label='out of distribution')\n",
    "ax.set(xlim=[0,2], xlabel='entropy', ylabel='density')\n",
    "ax.set_title('point predictions')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(b=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: post entropy histogram\n",
    "fig, ax = plt.subplots(figsize=(5,3.5))\n",
    "ax.hist(post_norm_entropy.data.numpy().squeeze(),\n",
    "        bins=100, range=(0,2), density=True, histtype='stepfilled',\n",
    "        alpha=0.7, label='in distribution')\n",
    "ax.hist(post_anom_entropy.data.numpy().squeeze(),\n",
    "        bins=100, range=(0,2), density=True, histtype='stepfilled',\n",
    "        alpha=0.7, label='out of distribution')\n",
    "ax.set(xlim=[0,2], xlabel='entropy', ylabel='density')\n",
    "ax.set_title('posterior predictive')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(b=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% normal MNIST\n",
    "# data_set = test_set\n",
    "# data_loader = test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% rotated MNIST\n",
    "max_rotation = 35\n",
    "preprocessor_with_noise = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=max_rotation, resample=PIL.Image.BILINEAR, fill=(0,)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "data_set = datasets.MNIST(data_path, train=False, transform=preprocessor_with_noise, download=True)\n",
    "data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% accuracies\n",
    "point_loss, point_acc = point_model.test(data_loader)\n",
    "post_acc = post_model.test_acc(data_loader, no_samples=1)\n",
    "print('Point acc.: {:.4f}'.format(point_acc))\n",
    "print('Post acc.: {:.4f}'.format(post_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% accuracy vs. confidence (point predictions)\n",
    "point_conf_edges, point_bin_accs = accuracy_vs_confidence(point_model,\n",
    "                                                          data_loader,\n",
    "                                                          likelihood_type='Categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: accuracy vs. confidence (point predictions)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(point_conf_edges[0:-1], point_bin_accs,\n",
    "       width=np.diff(point_conf_edges), align='edge',\n",
    "       alpha=0.7, edgecolor='black')\n",
    "ax.plot([0,1], [0,1], color='gray', linestyle='--')\n",
    "ax.set(xlim=[0,1], ylim=[0,1], xlabel='confidence', ylabel='accuracy')\n",
    "ax.set_title('point predictions')\n",
    "ax.grid(b=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% accuracy vs. confidence (posterior predictive)\n",
    "post_conf_edges, post_bin_accs = accuracy_vs_confidence(post_model,\n",
    "                                                        data_loader,\n",
    "                                                        likelihood_type='Categorical',\n",
    "                                                        no_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: accuracy vs. confidence (posterior predictive)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(post_conf_edges[0:-1], post_bin_accs,\n",
    "       width=np.diff(post_conf_edges), align='edge',\n",
    "       alpha=0.7, edgecolor='black')\n",
    "ax.plot([0,1], [0,1], color='gray', linestyle='--')\n",
    "ax.set(xlim=[0,1], ylim=[0,1], xlabel='confidence', ylabel='accuracy')\n",
    "ax.set_title('posterior predictive')\n",
    "ax.grid(b=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
