{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Bayesian inference (half-moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../torchutils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchutils import Classification\n",
    "from vartorch import VariationalClassification, VariationalLinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot data points\n",
    "def plot_data(X, y, colors=[plt.cm.Set1(1), plt.cm.Set1(0)], ax=None):\n",
    "    '''Plot two sampled classes on a two-dimensional plane.'''\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.scatter(X[y==0, 0], X[y==0, 1], color=colors[0], alpha=0.7, edgecolors='none', label='y=0')\n",
    "    ax.scatter(X[y==1, 0], X[y==1, 1], color=colors[1], alpha=0.7, edgecolors='none', label='y=1')\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot function over features\n",
    "def plot_function(function,\n",
    "                  levels=(0.1, 0.3, 0.5, 0.7, 0.9),\n",
    "                  x_limits=None,\n",
    "                  y_limits=None,\n",
    "                  colorbar=True,\n",
    "                  ax=None):\n",
    "    '''Plot a function of two features on the plane.'''\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    if x_limits is None:\n",
    "        x_limits = ax.get_xlim()\n",
    "    if y_limits is None:\n",
    "        y_limits = ax.get_ylim()\n",
    "    x_values = np.linspace(*x_limits, num=201)\n",
    "    y_values = np.linspace(*y_limits, num=201)\n",
    "    (X_values, Y_values) = np.meshgrid(x_values, y_values)\n",
    "    Z_values = function(np.stack((X_values.ravel(), Y_values.ravel()), axis=1)).reshape(X_values.shape)\n",
    "    im1 = ax.imshow(Z_values, origin='lower', extent=(*x_limits,*y_limits),\n",
    "                    interpolation='bicubic', cmap='Greys', alpha=0.4) # vmin=0, vmax=1\n",
    "    im2 = ax.contour(X_values, Y_values, Z_values, levels, colors='black', alpha=0.6)\n",
    "    if colorbar:\n",
    "        plt.colorbar(im1)\n",
    "    plt.clabel(im2, fmt='%1.2f')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half-moons data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% data\n",
    "no_samples = 500\n",
    "noise_level = 0.15\n",
    "X, y = make_moons(no_samples, shuffle=True, noise=noise_level)\n",
    "X[y==0,1] += 0.15\n",
    "X[y==1,1] += -0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: data\n",
    "x_limits = (-2, 3)\n",
    "y_limits = (-2, 2.5)\n",
    "fig, ax = plt.subplots()\n",
    "plot_data(X, y, ax=ax)\n",
    "ax.set(xlim=x_limits, ylim=y_limits)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(b=True, which='both', color='gray', alpha=0.2, linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% normalization\n",
    "original_scaler = StandardScaler()\n",
    "X_train_normalized = original_scaler.fit_transform(X_train)\n",
    "X_test_normalized = original_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% polynomial features\n",
    "polynomial_features = PolynomialFeatures(degree=5, interaction_only=False, include_bias=False)\n",
    "X_train_poly = polynomial_features.fit_transform(X_train_normalized)\n",
    "X_test_poly = polynomial_features.transform(X_test_normalized)\n",
    "no_features = X_train_poly.shape[1]\n",
    "print('Features:', polynomial_features.get_feature_names())\n",
    "print('No. features:', no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% normalization\n",
    "polynomial_scaler = StandardScaler()\n",
    "X_train_final = polynomial_scaler.fit_transform(X_train_poly)\n",
    "X_test_final = polynomial_scaler.transform(X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% tensors\n",
    "X_train_tensor = torch.tensor(X_train_final, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
    "X_test_tensor = torch.tensor(X_test_final, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% datasets and loaders\n",
    "train_set = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_set = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_set, batch_size=len(train_set), shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=len(test_set), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% model (logistic regression)\n",
    "model = nn.Linear(in_features=no_features, out_features=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% standard model\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "point_model = Classification(model, criterion, optimizer, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% training\n",
    "point_history = point_model.training(no_epochs=500, log_interval=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% testing\n",
    "point_train_loss, point_train_acc = point_model.test(train_loader)\n",
    "point_test_loss, point_test_acc = point_model.test(test_loader)\n",
    "print('Train loss: {:.4f}'.format(point_train_loss))\n",
    "print('Test loss: {:.4f}'.format(point_test_loss))\n",
    "print('Train acc.: {:.4f}'.format(point_train_acc))\n",
    "print('Test acc.: {:.4f}'.format(point_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: training history\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.array(point_history['train_loss']), label='training', alpha=0.7)\n",
    "ax.plot(np.array(point_history['test_loss']), label='testing', alpha=0.7)\n",
    "ax.set(xlabel='epoch', ylabel='loss')\n",
    "ax.set_xlim([0, point_history['no_epochs']])\n",
    "ax.legend()\n",
    "ax.grid(b=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% model (variational logistic regression)\n",
    "model = VariationalLinear(in_features=no_features, out_features=1, weight_std=5)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% variational inference\n",
    "post_model = VariationalClassification(model, likelihood_type='Bernoulli')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "post_model.compile_for_training(optimizer, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% training\n",
    "post_history = post_model.training(no_epochs=500, no_samples=20, log_interval=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% testing\n",
    "post_train_loss = post_model.test_loss(train_loader)\n",
    "post_train_acc = post_model.test_acc(train_loader)\n",
    "post_test_loss = post_model.test_loss(test_loader)\n",
    "post_test_acc = post_model.test_acc(test_loader)\n",
    "print('Train loss: {:.4f}'.format(post_train_loss))\n",
    "print('Test loss: {:.4f}'.format(post_test_loss))\n",
    "print('Train acc.: {:.4f}'.format(post_train_acc))\n",
    "print('Test acc.: {:.4f}'.format(post_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: training history\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(-np.array(post_history['train_loss']), label='training', alpha=0.7)\n",
    "ax.plot(-np.array(post_history['test_loss']), label='testing', alpha=0.7)\n",
    "ax.set(xlabel='epoch', ylabel='ELBO')\n",
    "ax.set_xlim([0, post_history['no_epochs']])\n",
    "ax.legend()\n",
    "ax.grid(b=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% function definitions\n",
    "def transform_features(x):\n",
    "    '''Transform features.'''\n",
    "    x_normalized = original_scaler.transform(x)\n",
    "    x_poly = polynomial_features.transform(x_normalized)\n",
    "    x_final = polynomial_scaler.transform(x_poly)\n",
    "    x_tensor = torch.tensor(x_final, dtype=torch.float32)\n",
    "    return x_tensor\n",
    "\n",
    "def point_prediction(x):\n",
    "    '''Compute normal point predictions.'''\n",
    "    x_tensor = transform_features(x)\n",
    "    point_model.train(False)\n",
    "    with torch.no_grad():\n",
    "        point_logits = point_model.predict(x_tensor.to(point_model.device)).cpu()\n",
    "        point_probs = torch.sigmoid(point_logits)\n",
    "    return point_probs.data.numpy()\n",
    "\n",
    "def posterior_mean(x):\n",
    "    '''Predict with posterior mean weights.'''\n",
    "    x_tensor = transform_features(x)\n",
    "    post_model.sample(False)\n",
    "    post_model.train(False)\n",
    "    with torch.no_grad():\n",
    "        point_logits = post_model.predict(x_tensor.to(post_model.device)).cpu()\n",
    "        point_probs = torch.sigmoid(point_logits)\n",
    "    return point_probs.data.numpy()\n",
    "\n",
    "def posterior_predictive(x, no_samples=1000):\n",
    "    '''Predict according to the posterior predictive distribution.'''\n",
    "    x_tensor = transform_features(x)\n",
    "    post_model.sample(True)\n",
    "    post_model.train(False)\n",
    "    with torch.no_grad():\n",
    "        sampled_logits = post_model.predict(x_tensor.to(post_model.device), no_samples).cpu()\n",
    "        sampled_probs = torch.sigmoid(sampled_logits)\n",
    "    post_mean = torch.mean(sampled_probs, axis=-1)\n",
    "    return post_mean.data.numpy()\n",
    "\n",
    "def posterior_uncertainty(x, no_samples=1000):\n",
    "    '''Compute the uncertainty associated with the posterior predictive.'''\n",
    "    x_tensor = transform_features(x)\n",
    "    post_model.sample(True)\n",
    "    post_model.train(False)\n",
    "    with torch.no_grad():\n",
    "        sampled_logits = post_model.predict(x_tensor.to(post_model.device), no_samples).cpu()\n",
    "        sampled_probs = torch.sigmoid(sampled_logits)\n",
    "    post_std = torch.std(sampled_probs, axis=-1)\n",
    "    return post_std.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: point predictions\n",
    "x_limits = (-2, 3)\n",
    "y_limits = (-2, 2.5)\n",
    "fig, ax = plt.subplots()\n",
    "plot_data(X_train, y_train, ax=ax)\n",
    "ax.set(xlim=x_limits, ylim=y_limits)\n",
    "plot_function(point_prediction, levels=(0.3, 0.5, 0.7), ax=ax)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_title('point predictions')\n",
    "ax.legend(loc='upper left')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: posterior mean\n",
    "x_limits = (-2, 3)\n",
    "y_limits = (-2, 2.5)\n",
    "fig, ax = plt.subplots()\n",
    "plot_data(X_train, y_train, ax=ax)\n",
    "ax.set(xlim=x_limits, ylim=y_limits)\n",
    "plot_function(posterior_mean, levels=(0.3, 0.5, 0.7), ax=ax)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_title('posterior mean')\n",
    "ax.legend(loc='upper left')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: posterior predictive\n",
    "x_limits = (-2, 3)\n",
    "y_limits = (-2, 2.5)\n",
    "fig, ax = plt.subplots()\n",
    "plot_data(X_train, y_train, ax=ax)\n",
    "ax.set(xlim=x_limits, ylim=y_limits)\n",
    "plot_function(posterior_predictive, levels=(0.1, 0.3, 0.5, 0.7, 0.9), ax=ax)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_title('posterior predictive')\n",
    "ax.legend(loc='upper left')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: posterior uncertainty\n",
    "x_limits = (-2, 3)\n",
    "y_limits = (-2, 2.5)\n",
    "fig, ax = plt.subplots()\n",
    "plot_data(X_train, y_train, ax=ax)\n",
    "ax.set(xlim=x_limits, ylim=y_limits)\n",
    "plot_function(posterior_uncertainty, levels=np.linspace(0.1, 0.9, 9), ax=ax)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_title('posterior uncertainty')\n",
    "ax.legend(loc='upper left')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
