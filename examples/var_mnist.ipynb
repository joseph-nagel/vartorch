{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Bayesian inference (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../torchutils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchutils import Classification, confusion_matrix\n",
    "from vartorch import \\\n",
    "    VariationalClassification, \\\n",
    "    VariationalLinear, \\\n",
    "    anomaly_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% transformations\n",
    "preprocessor = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% datasets\n",
    "data_path = pathlib.Path.home() / 'Data'\n",
    "train_set = datasets.MNIST(data_path,\n",
    "                           train=True,\n",
    "                           transform=preprocessor,\n",
    "                           download=True)\n",
    "test_set = datasets.MNIST(data_path,\n",
    "                          train=False,\n",
    "                          transform=preprocessor,\n",
    "                          download=True)\n",
    "print('No. train images:', len(train_set))\n",
    "print('No. test images:', len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(test_set,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "print('No. train batches:', len(train_loader))\n",
    "print('No. test batches:', len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% example images\n",
    "images, labels = next(iter(train_loader))\n",
    "print('Images shape:', images.shape)\n",
    "print('Labels shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: example images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(5,3))\n",
    "for idx, ax in enumerate(axes.ravel()):\n",
    "    image = images[idx,0].numpy()\n",
    "    ax.imshow(image.clip(0,1), cmap='gray')\n",
    "    ax.set_title(train_set.classes[labels[idx]])\n",
    "    ax.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% model (logistic regression)\n",
    "# model1 = nn.Sequential(\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(in_features=28*28, out_features=10),\n",
    "# )\n",
    "# print(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% model (small CNN with linear classifier)\n",
    "model1 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(5,5), padding=2),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=4, out_channels=8, kernel_size=(5,5), padding=2),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=7*7*8, out_features=10)\n",
    ")\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% standard model\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
    "point_model = Classification(model1, criterion, optimizer, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% training\n",
    "point_history = point_model.training(no_epochs=10, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: training history\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(np.array(point_history['train_loss']), label='training', alpha=0.7)\n",
    "ax.plot(np.array(point_history['test_loss']), label='testing', alpha=0.7)\n",
    "ax.set(xlabel='epoch', ylabel='loss')\n",
    "ax.set_xlim([0, point_history['no_epochs']])\n",
    "ax.legend()\n",
    "ax.grid(b=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% final loss/accuracy\n",
    "point_train_loss, point_train_acc = point_model.test(train_loader)\n",
    "point_test_loss, point_test_acc = point_model.test(test_loader)\n",
    "print('Train loss: {:.4f}'.format(point_train_loss))\n",
    "print('Test loss: {:.4f}'.format(point_test_loss))\n",
    "print('Train acc.: {:.4f}'.format(point_train_acc))\n",
    "print('Test acc.: {:.4f}'.format(point_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% confusion matrix\n",
    "confmat = confusion_matrix(point_model, test_loader)\n",
    "print('Confusion matrix:\\n{}'.format(confmat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% model (variational logistic regression)\n",
    "# model2 = nn.Sequential(\n",
    "#     nn.Flatten(),\n",
    "#     VariationalLinear(in_features=28*28, out_features=10),\n",
    "# )\n",
    "# print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% model (small CNN with variational linear classifier)\n",
    "model2 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(5,5), padding=2),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=4, out_channels=8, kernel_size=(5,5), padding=2),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    VariationalLinear(in_features=7*7*8, out_features=10)\n",
    ")\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% variational inference\n",
    "post_model = VariationalClassification(model2, likelihood_type='Categorical')\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
    "post_model.compile_for_training(optimizer, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% training\n",
    "post_history = post_model.training(no_epochs=20, no_samples=10, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: training history\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(-np.array(post_history['train_loss']), label='training', alpha=0.7)\n",
    "ax.plot(-np.array(post_history['test_loss']), label='testing', alpha=0.7)\n",
    "ax.set(xlabel='epoch', ylabel='ELBO')\n",
    "ax.set_xlim([0, post_history['no_epochs']])\n",
    "ax.legend()\n",
    "ax.grid(b=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% final loss/accuracy\n",
    "post_train_loss = post_model.test_loss(train_loader)\n",
    "post_train_acc = post_model.test_acc(train_loader)\n",
    "post_test_loss = post_model.test_loss(test_loader)\n",
    "post_test_acc = post_model.test_acc(test_loader)\n",
    "print('Train loss: {:.4f}'.format(post_train_loss))\n",
    "print('Test loss: {:.4f}'.format(post_test_loss))\n",
    "print('Train acc.: {:.4f}'.format(post_train_acc))\n",
    "print('Test acc.: {:.4f}'.format(post_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% confusion matrix\n",
    "confmat = confusion_matrix(post_model, test_loader, no_samples=100)\n",
    "print('Confusion matrix:\\n{}'.format(confmat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% datasets and loaders\n",
    "norm_set = test_set\n",
    "norm_loader = test_loader\n",
    "anom_set = datasets.KMNIST(data_path, train=False, transform=preprocessor, download=True) # KMNIST\n",
    "# anom_set = datasets.FashionMNIST(data_path, train=False, transform=preprocessor, download=True) # FashionMNIST\n",
    "# anom_set = TensorDataset(torch.rand(batch_size, 1, 28, 28), # random noise\n",
    "#                          torch.zeros((batch_size,), dtype=torch.int64))\n",
    "anom_loader = DataLoader(anom_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% example data\n",
    "norm_images, norm_labels = next(iter(norm_loader))\n",
    "anom_images, anom_labels = next(iter(anom_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% standard point predictions\n",
    "point_model.train(False)\n",
    "with torch.no_grad():\n",
    "    point_norm_probs = point_model.predict_proba(norm_images.to(point_model.device)).cpu()\n",
    "    point_norm_entropy = dist.Categorical(probs=point_norm_probs).entropy()\n",
    "    point_anom_probs = point_model.predict_proba(anom_images.to(point_model.device)).cpu()\n",
    "    point_anom_entropy = dist.Categorical(probs=point_anom_probs).entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% posterior sample predictions\n",
    "no_samples = 500\n",
    "post_model.sample(True)\n",
    "post_model.train(False)\n",
    "with torch.no_grad():\n",
    "    sampled_norm_logits = post_model.predict(norm_images.to(post_model.device), no_samples).cpu()\n",
    "    sampled_norm_probs = torch.softmax(sampled_norm_logits, dim=1)\n",
    "    post_norm_probs = torch.mean(sampled_norm_probs, axis=-1)\n",
    "    post_norm_entropy = dist.Categorical(probs=post_norm_probs).entropy()\n",
    "    sampled_anom_logits = post_model.predict(anom_images.to(post_model.device), no_samples).cpu()\n",
    "    sampled_anom_probs = torch.softmax(sampled_anom_logits, dim=1)\n",
    "    post_anom_probs = torch.mean(sampled_anom_probs, axis=-1)\n",
    "    post_anom_entropy = dist.Categorical(probs=post_anom_probs).entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: point predictions (in distribution)\n",
    "plot_ids = np.random.permutation(np.arange(len(images))) # random\n",
    "# plot_ids = torch.argsort(point_norm_entropy, descending=False).data.numpy() # lowest entropy\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(5,6))\n",
    "for idx, (ax1, ax2) in zip(plot_ids[:axes.shape[0]], axes):\n",
    "    # image\n",
    "    image = norm_images[idx,0].numpy()\n",
    "    ax1.imshow(image.clip(0,1), cmap='gray')\n",
    "    ax1.set_title('{}'.format(norm_set.classes[norm_labels[idx]])\n",
    "                  if hasattr(norm_set, 'classes') else 'random')\n",
    "    ax1.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "    # probabilities\n",
    "    ax2.bar(np.arange(10), point_norm_probs.data.numpy()[idx])\n",
    "    ax2.set_title('$\\pi(c|x,\\hat{w})$')\n",
    "    ax2.set(xticks=np.arange(10), ylim=[0,1], xlabel='c')\n",
    "    # ax2.text(0, 0.75, 'entropy: {:.2f}'.format(point_norm_entropy[idx]), alpha=0.5)\n",
    "fig.suptitle('point predictions (in distribution)')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: point predictions (out of distribution)\n",
    "plot_ids = np.random.permutation(np.arange(len(images))) # random\n",
    "# plot_ids = torch.argsort(point_anom_entropy, descending=False).data.numpy() # lowest entropy\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(5,6))\n",
    "for idx, (ax1, ax2) in zip(plot_ids[:axes.shape[0]], axes):\n",
    "    # image\n",
    "    image = anom_images[idx,0].numpy()\n",
    "    ax1.imshow(image.clip(0,1), cmap='gray')\n",
    "    ax1.set_title('{}'.format(anom_set.classes[anom_labels[idx]])\n",
    "                  if hasattr(anom_set, 'classes') else 'random')\n",
    "    ax1.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "    # probabilities\n",
    "    ax2.bar(np.arange(10), point_anom_probs.data.numpy()[idx])\n",
    "    ax2.set_title('$\\pi(c|x,\\hat{w})$')\n",
    "    ax2.set(xticks=np.arange(10), ylim=[0,1], xlabel='c')\n",
    "    # ax2.text(0, 0.75, 'entropy: {:.2f}'.format(point_anom_entropy[idx]), alpha=0.5)\n",
    "fig.suptitle('point predictions (out of distribution)')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: posterior predictions (in distribution)\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(8,6))\n",
    "for idx, (ax1, ax2, ax3) in zip(plot_ids[:axes.shape[0]], axes):\n",
    "    # image\n",
    "    image = norm_images[idx,0].numpy()\n",
    "    ax1.imshow(image.clip(0,1), cmap='gray')\n",
    "    ax1.set_title('{}'.format(norm_set.classes[norm_labels[idx]])\n",
    "                  if hasattr(norm_set, 'classes') else 'random noise')\n",
    "    ax1.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "    # violin plot\n",
    "    # ax2.violinplot(sampled_norm_probs[idx,:,:], positions=np.arange(10))\n",
    "    # ax2.set_title('$\\pi(c|x,w)$, $w$ from $\\pi(w|\\mathcal{D})$')\n",
    "    # ax2.set(xticks=np.arange(10), ylim=[0,1], xlabel='c')\n",
    "    # histogram\n",
    "    highest_ids = post_norm_probs[idx].data.numpy().argsort()[::-1][:3]\n",
    "    for highest_idx in highest_ids:\n",
    "        ax2.hist(sampled_norm_probs[idx,highest_idx,:].data.numpy(), bins=50,\n",
    "                 range=[0,1], density=True, histtype='stepfilled', alpha=0.5)\n",
    "    ax2.set_title('$\\pi(c|x,w)$, $w$ from $\\pi(w|\\mathcal{D})$')\n",
    "    ax2.set_xlim([0,1])\n",
    "    ax2.legend(['c={}'.format(c) for c in highest_ids], loc='upper center')\n",
    "    ax2.grid(b=True, which='both', color='lightgray', linestyle='-')\n",
    "    ax2.set_axisbelow(True)\n",
    "    # posterior predictive\n",
    "    ax3.bar(np.arange(10), post_norm_probs[idx].data.numpy())\n",
    "    ax3.set_title('$\\pi(c|x,\\mathcal{D}) = \\int \\pi(c|x,w) \\pi(w|\\mathcal{D}) dw$')\n",
    "    ax3.set(xticks=np.arange(10), ylim=[0,1], xlabel='c')\n",
    "    # ax3.text(0, 0.75, 'entropy: {:.2f}'.format(post_norm_entropy[idx]), alpha=0.5)\n",
    "fig.suptitle('posterior predictions (in distribution)')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: posterior predictions (out of distribution)\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(8,6))\n",
    "for idx, (ax1, ax2, ax3) in zip(plot_ids[:axes.shape[0]], axes):\n",
    "    # image\n",
    "    image = anom_images[idx,0].numpy()\n",
    "    ax1.imshow(image.clip(0,1), cmap='gray')\n",
    "    ax1.set_title('{}'.format(anom_set.classes[anom_labels[idx]])\n",
    "                  if hasattr(anom_set, 'classes') else 'random noise')\n",
    "    ax1.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "    # violin plot\n",
    "    # ax2.violinplot(sampled_anom_probs[idx,:,:], positions=np.arange(10))\n",
    "    # ax2.set_title('$\\pi(c|x,w)$, $w$ from $\\pi(w|\\mathcal{D})$')\n",
    "    # ax2.set(xticks=np.arange(10), ylim=[0,1], xlabel='c')\n",
    "    # histogram\n",
    "    highest_ids = post_anom_probs[idx].data.numpy().argsort()[::-1][:3]\n",
    "    for highest_idx in highest_ids:\n",
    "        ax2.hist(sampled_anom_probs[idx,highest_idx,:].data.numpy(), bins=50,\n",
    "                 range=[0,1], density=True, histtype='stepfilled', alpha=0.5)\n",
    "    ax2.set_title('$\\pi(c|x,w)$, $w$ from $\\pi(w|\\mathcal{D})$')\n",
    "    ax2.set_xlim([0,1])\n",
    "    ax2.legend(['c={}'.format(c) for c in highest_ids], loc='upper center')\n",
    "    ax2.grid(b=True, which='both', color='lightgray', linestyle='-')\n",
    "    ax2.set_axisbelow(True)\n",
    "    # posterior predictive\n",
    "    ax3.bar(np.arange(10), post_anom_probs[idx].data.numpy())\n",
    "    ax3.set_title('$\\pi(c|x,\\mathcal{D}) = \\int \\pi(c|x,w) \\pi(w|\\mathcal{D}) dw$')\n",
    "    ax3.set(xticks=np.arange(10), ylim=[0,1], xlabel='c')\n",
    "    # ax3.text(0, 0.75, 'entropy: {:.2f}'.format(post_anom_entropy[idx]), alpha=0.5)\n",
    "fig.suptitle('posterior predictions (out of distribution)')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-distribution detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% datasets and loaders\n",
    "norm_loader = test_loader # MNIST\n",
    "anom_set = datasets.KMNIST(data_path, train=False, transform=preprocessor, download=True) # KMNIST\n",
    "# anom_set = datasets.FashionMNIST(data_path, train=False, transform=preprocessor, download=True) # FashionMNIST\n",
    "# anom_set = TensorDataset(torch.rand(batch_size, 1, 28, 28), # random noise\n",
    "#                          torch.zeros((batch_size,), dtype=torch.int64))\n",
    "anom_loader = DataLoader(anom_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% anomaly scores\n",
    "point_norm_entropy = anomaly_score(point_model, norm_loader, mode='entropy')\n",
    "point_anom_entropy = anomaly_score(point_model, anom_loader, mode='entropy')\n",
    "post_norm_entropy = anomaly_score(post_model, norm_loader, mode='entropy', no_samples=100)\n",
    "post_anom_entropy = anomaly_score(post_model, anom_loader, mode='entropy', no_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: point entropy histogram\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.hist(point_norm_entropy, bins=100, range=(0,2), density=True,\n",
    "        histtype='stepfilled', alpha=0.7, label='in distribution')\n",
    "ax.hist(point_anom_entropy, bins=100, range=(0,2), density=True,\n",
    "        histtype='stepfilled', alpha=0.7, label='out of distribution')\n",
    "ax.set(xlim=[0,2], xlabel='entropy', ylabel='density')\n",
    "ax.set_title('point predictions')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(b=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot: posterior entropy histogram\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.hist(post_norm_entropy, bins=100, range=(0,2), density=True,\n",
    "        histtype='stepfilled', alpha=0.7, label='in distribution')\n",
    "ax.hist(post_anom_entropy, bins=100, range=(0,2), density=True,\n",
    "        histtype='stepfilled', alpha=0.7, label='out of distribution')\n",
    "ax.set(xlim=[0,2], xlabel='entropy', ylabel='density')\n",
    "ax.set_title('posterior predictive')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(b=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
