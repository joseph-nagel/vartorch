{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Bayesian inference (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../torchutils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torchutils import Classification, confusion_matrix\n",
    "\n",
    "from vartorch import (\n",
    "    VariationalClassification,\n",
    "    VariationalLinear,\n",
    "    anomaly_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds manually\n",
    "np.random.seed(1223334444)\n",
    "_ = torch.manual_seed(55555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.home() / 'Data'\n",
    "\n",
    "train_set = datasets.MNIST(\n",
    "    data_path,\n",
    "    train=True,\n",
    "    transform=preprocessor,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "val_set = datasets.MNIST(\n",
    "    data_path,\n",
    "    train=False,\n",
    "    transform=preprocessor,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "print('No. train images:', len(train_set))\n",
    "print('No. val. images:', len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print('No. train batches:', len(train_loader))\n",
    "print('No. val. batches:', len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print('Images shape:', images.shape)\n",
    "print('Labels shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(5, 3))\n",
    "for idx, ax in enumerate(axes.ravel()):\n",
    "    image = images[idx,0].numpy()\n",
    "    ax.imshow(image.clip(0, 1), cmap='gray')\n",
    "    ax.set_title(train_set.classes[labels[idx]])\n",
    "    ax.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logistic regression model\n",
    "# model1 = nn.Sequential(\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(in_features=28*28, out_features=10),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create small CNN with linear classifier\n",
    "model1 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(5, 5), padding=2),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=4, out_channels=8, kernel_size=(5, 5), padding=2),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=7*7*8, out_features=10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
    "\n",
    "point_model = Classification(\n",
    "    model1,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "point_history = point_model.training(\n",
    "    num_epochs=100,\n",
    "    log_interval=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.plot(np.array(point_history['train_loss']), label='train', alpha=0.7)\n",
    "ax.plot(np.array(point_history['val_loss']), label='val.', alpha=0.7)\n",
    "ax.set(xlabel='epoch', ylabel='loss')\n",
    "ax.set_xlim((0, point_history['num_epochs']))\n",
    "ax.legend()\n",
    "ax.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_train_loss, point_train_acc = point_model.test(train_loader)\n",
    "point_val_loss, point_val_acc = point_model.test(val_loader)\n",
    "\n",
    "print('Train loss: {:.4f}'.format(point_train_loss))\n",
    "print('Val. loss: {:.4f}'.format(point_val_loss))\n",
    "print('\\nTrain acc.: {:.4f}'.format(point_train_acc))\n",
    "print('Val. acc.: {:.4f}'.format(point_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat = confusion_matrix(point_model, val_loader)\n",
    "print('Confusion matrix:\\n{}'.format(confmat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variational logistic regression model\n",
    "# model2 = nn.Sequential(\n",
    "#     nn.Flatten(),\n",
    "#     VariationalLinear(in_features=28*28, out_features=10),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create small CNN with variational linear classifier\n",
    "model2 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(5, 5), padding=2),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=4, out_channels=8, kernel_size=(5, 5), padding=2),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    VariationalLinear(in_features=7*7*8, out_features=10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
    "\n",
    "post_model = VariationalClassification(\n",
    "    model2,\n",
    "    likelihood_type='Categorical'\n",
    ")\n",
    "\n",
    "post_model.compile_for_training(\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "post_history = post_model.training(\n",
    "    num_epochs=100,\n",
    "    num_samples=10,\n",
    "    log_interval=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.plot(-np.array(post_history['train_loss']), label='train', alpha=0.7)\n",
    "ax.plot(-np.array(post_history['val_loss']), label='val.', alpha=0.7)\n",
    "ax.set(xlabel='epoch', ylabel='ELBO')\n",
    "ax.set_xlim((0, post_history['num_epochs']))\n",
    "ax.legend()\n",
    "ax.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_train_loss = post_model.test_loss(train_loader)\n",
    "post_train_acc = post_model.test_acc(train_loader)\n",
    "\n",
    "post_val_loss = post_model.test_loss(val_loader)\n",
    "post_val_acc = post_model.test_acc(val_loader)\n",
    "\n",
    "print('Train loss: {:.4f}'.format(post_train_loss))\n",
    "print('Val. loss: {:.4f}'.format(post_val_loss))\n",
    "print('\\nTrain acc.: {:.4f}'.format(post_train_acc))\n",
    "print('Val. acc.: {:.4f}'.format(post_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat = confusion_matrix(post_model, val_loader, num_samples=100)\n",
    "print('Confusion matrix:\\n{}'.format(confmat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_set = val_set\n",
    "norm_loader = val_loader\n",
    "\n",
    "anom_set = datasets.KMNIST(data_path, train=False, transform=preprocessor, download=True)\n",
    "# anom_set = datasets.FashionMNIST(data_path, train=False, transform=preprocessor, download=True)\n",
    "# anom_set = TensorDataset(torch.rand(batch_size, 1, 28, 28), torch.zeros((batch_size,), dtype=torch.int64))\n",
    "\n",
    "anom_loader = DataLoader(anom_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "norm_images, norm_labels = next(iter(norm_loader))\n",
    "anom_images, anom_labels = next(iter(anom_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_model.train(False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    point_norm_probs = point_model.predict_proba(norm_images.to(point_model.device)).cpu()\n",
    "    point_norm_entropy = dist.Categorical(probs=point_norm_probs).entropy()\n",
    "\n",
    "    point_anom_probs = point_model.predict_proba(anom_images.to(point_model.device)).cpu()\n",
    "    point_anom_entropy = dist.Categorical(probs=point_anom_probs).entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 500\n",
    "\n",
    "post_model.sample(True)\n",
    "post_model.train(False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sampled_norm_logits = post_model.predict(norm_images.to(post_model.device), num_samples).cpu()\n",
    "    sampled_norm_probs = torch.softmax(sampled_norm_logits, dim=1)\n",
    "\n",
    "    post_norm_probs = torch.mean(sampled_norm_probs, axis=-1)\n",
    "    post_norm_entropy = dist.Categorical(probs=post_norm_probs).entropy()\n",
    "\n",
    "    sampled_anom_logits = post_model.predict(anom_images.to(post_model.device), num_samples).cpu()\n",
    "    sampled_anom_probs = torch.softmax(sampled_anom_logits, dim=1)\n",
    "\n",
    "    post_anom_probs = torch.mean(sampled_anom_probs, axis=-1)\n",
    "    post_anom_entropy = dist.Categorical(probs=post_anom_probs).entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot point predictions (in distribution)\n",
    "plot_ids = np.random.permutation(np.arange(len(images))) # random\n",
    "# plot_ids = torch.argsort(point_norm_entropy, descending=False).data.numpy() # lowest entropy\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(5, 6))\n",
    "for idx, (ax1, ax2) in zip(plot_ids[:axes.shape[0]], axes):\n",
    "    # image\n",
    "    image = norm_images[idx,0].numpy()\n",
    "    ax1.imshow(image.clip(0, 1), cmap='gray')\n",
    "    ax1.set_title(\n",
    "        '{}'.format(norm_set.classes[norm_labels[idx]])\n",
    "        if hasattr(norm_set, 'classes') else 'random'\n",
    "    )\n",
    "    ax1.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "\n",
    "    # probabilities\n",
    "    ax2.bar(np.arange(10), point_norm_probs.data.numpy()[idx])\n",
    "    ax2.set_title('$\\pi(c|x,\\hat{w})$')\n",
    "    ax2.set(xticks=np.arange(10), ylim=(0, 1), xlabel='c')\n",
    "    # ax2.text(0, 0.75, 'entropy: {:.2f}'.format(point_norm_entropy[idx]), alpha=0.5)\n",
    "\n",
    "fig.suptitle('Point predictions (in distribution)')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot point predictions (out of distribution)\n",
    "plot_ids = np.random.permutation(np.arange(len(images))) # random\n",
    "# plot_ids = torch.argsort(point_anom_entropy, descending=False).data.numpy() # lowest entropy\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(5, 6))\n",
    "for idx, (ax1, ax2) in zip(plot_ids[:axes.shape[0]], axes):\n",
    "    # image\n",
    "    image = anom_images[idx,0].numpy()\n",
    "    ax1.imshow(image.clip(0, 1), cmap='gray')\n",
    "    ax1.set_title(\n",
    "        '{}'.format(anom_set.classes[anom_labels[idx]])\n",
    "        if hasattr(anom_set, 'classes') else 'random'\n",
    "    )\n",
    "    ax1.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "\n",
    "    # probabilities\n",
    "    ax2.bar(np.arange(10), point_anom_probs.data.numpy()[idx])\n",
    "    ax2.set_title('$\\pi(c|x,\\hat{w})$')\n",
    "    ax2.set(xticks=np.arange(10), ylim=(0, 1), xlabel='c')\n",
    "    # ax2.text(0, 0.75, 'entropy: {:.2f}'.format(point_anom_entropy[idx]), alpha=0.5)\n",
    "\n",
    "fig.suptitle('Point predictions (out of distribution)')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot posterior predictions (in distribution)\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(8, 6))\n",
    "\n",
    "for idx, (ax1, ax2, ax3) in zip(plot_ids[:axes.shape[0]], axes):\n",
    "    # image\n",
    "    image = norm_images[idx,0].numpy()\n",
    "    ax1.imshow(image.clip(0, 1), cmap='gray')\n",
    "    ax1.set_title(\n",
    "        '{}'.format(norm_set.classes[norm_labels[idx]])\n",
    "        if hasattr(norm_set, 'classes') else 'random noise'\n",
    "    )\n",
    "    ax1.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "\n",
    "    # violin plot\n",
    "    # ax2.violinplot(sampled_norm_probs[idx,:,:], positions=np.arange(10))\n",
    "    # ax2.set_title('$\\pi(c|x,w)$, $w$ from $\\pi(w|\\mathcal{D})$')\n",
    "    # ax2.set(xticks=np.arange(10), ylim=[0,1], xlabel='c')\n",
    "\n",
    "    # histogram\n",
    "    highest_ids = post_norm_probs[idx].data.numpy().argsort()[::-1][:3]\n",
    "    for highest_idx in highest_ids:\n",
    "        ax2.hist(\n",
    "            sampled_norm_probs[idx,highest_idx,:].data.numpy(), bins=50,\n",
    "            range=(0, 1), density=True, histtype='stepfilled', alpha=0.5\n",
    "        )\n",
    "    ax2.set_title('$\\pi(c|x,w)$, $w$ from $\\pi(w|\\mathcal{D})$')\n",
    "    ax2.set_xlim((0, 1))\n",
    "    ax2.legend(['c={}'.format(c) for c in highest_ids], loc='upper center')\n",
    "    ax2.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "    ax2.set_axisbelow(True)\n",
    "\n",
    "    # posterior predictive\n",
    "    ax3.bar(np.arange(10), post_norm_probs[idx].data.numpy())\n",
    "    ax3.set_title('$\\pi(c|x,\\mathcal{D}) = \\int \\pi(c|x,w) \\pi(w|\\mathcal{D}) dw$')\n",
    "    ax3.set(xticks=np.arange(10), ylim=(0, 1), xlabel='c')\n",
    "    # ax3.text(0, 0.75, 'entropy: {:.2f}'.format(post_norm_entropy[idx]), alpha=0.5)\n",
    "\n",
    "fig.suptitle('Posterior predictions (in distribution)')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot posterior predictions (out of distribution)\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(8, 6))\n",
    "\n",
    "for idx, (ax1, ax2, ax3) in zip(plot_ids[:axes.shape[0]], axes):\n",
    "    # image\n",
    "    image = anom_images[idx,0].numpy()\n",
    "    ax1.imshow(image.clip(0, 1), cmap='gray')\n",
    "    ax1.set_title(\n",
    "        '{}'.format(anom_set.classes[anom_labels[idx]])\n",
    "        if hasattr(anom_set, 'classes') else 'random noise'\n",
    "    )\n",
    "    ax1.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "\n",
    "    # violin plot\n",
    "    # ax2.violinplot(sampled_anom_probs[idx,:,:], positions=np.arange(10))\n",
    "    # ax2.set_title('$\\pi(c|x,w)$, $w$ from $\\pi(w|\\mathcal{D})$')\n",
    "    # ax2.set(xticks=np.arange(10), ylim=[0,1], xlabel='c')\n",
    "\n",
    "    # histogram\n",
    "    highest_ids = post_anom_probs[idx].data.numpy().argsort()[::-1][:3]\n",
    "    for highest_idx in highest_ids:\n",
    "        ax2.hist(\n",
    "            sampled_anom_probs[idx,highest_idx,:].data.numpy(), bins=50,\n",
    "            range=[0,1], density=True, histtype='stepfilled', alpha=0.5\n",
    "        )\n",
    "    ax2.set_title('$\\pi(c|x,w)$, $w$ from $\\pi(w|\\mathcal{D})$')\n",
    "    ax2.set_xlim((0, 1))\n",
    "    ax2.legend(['c={}'.format(c) for c in highest_ids], loc='upper center')\n",
    "    ax2.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "    ax2.set_axisbelow(True)\n",
    "\n",
    "    # posterior predictive\n",
    "    ax3.bar(np.arange(10), post_anom_probs[idx].data.numpy())\n",
    "    ax3.set_title('$\\pi(c|x,\\mathcal{D}) = \\int \\pi(c|x,w) \\pi(w|\\mathcal{D}) dw$')\n",
    "    ax3.set(xticks=np.arange(10), ylim=(0, 1), xlabel='c')\n",
    "    # ax3.text(0, 0.75, 'entropy: {:.2f}'.format(post_anom_entropy[idx]), alpha=0.5)\n",
    "\n",
    "fig.suptitle('Posterior predictions (out of distribution)')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-distribution detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_loader = val_loader\n",
    "\n",
    "anom_set = datasets.KMNIST(data_path, train=False, transform=preprocessor, download=True)\n",
    "# anom_set = datasets.FashionMNIST(data_path, train=False, transform=preprocessor, download=True)\n",
    "# anom_set = TensorDataset(torch.rand(batch_size, 1, 28, 28), torch.zeros((batch_size,), dtype=torch.int64))\n",
    "\n",
    "anom_loader = DataLoader(anom_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_norm_entropy = anomaly_score(point_model, norm_loader, mode='entropy')\n",
    "point_anom_entropy = anomaly_score(point_model, anom_loader, mode='entropy')\n",
    "\n",
    "post_norm_entropy = anomaly_score(post_model, norm_loader, mode='entropy', num_samples=100)\n",
    "post_anom_entropy = anomaly_score(post_model, anom_loader, mode='entropy', num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot point entropy histogram\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.hist(\n",
    "    point_norm_entropy, bins=100, range=(0, 2), density=True,\n",
    "    histtype='stepfilled', alpha=0.7, label='in distribution'\n",
    ")\n",
    "ax.hist(\n",
    "    point_anom_entropy, bins=100, range=(0, 2), density=True,\n",
    "    histtype='stepfilled', alpha=0.7, label='out of distribution'\n",
    ")\n",
    "ax.set(xlim=(0, 2), xlabel='entropy', ylabel='density')\n",
    "ax.set_title('Point predictions')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot posterior entropy histogram\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.hist(\n",
    "    post_norm_entropy, bins=100, range=(0, 2), density=True,\n",
    "    histtype='stepfilled', alpha=0.7, label='in distribution'\n",
    ")\n",
    "ax.hist(\n",
    "    post_anom_entropy, bins=100, range=(0, 2), density=True,\n",
    "    histtype='stepfilled', alpha=0.7, label='out of distribution'\n",
    ")\n",
    "ax.set(xlim=(0, 2), xlabel='entropy', ylabel='density')\n",
    "ax.set_title('Posterior predictive')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
