{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Bayesian inference (half-moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../torchutils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchutils import Classification\n",
    "\n",
    "from vartorch import VariationalClassification, VariationalLinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y, colors=[plt.cm.Set1(1), plt.cm.Set1(0)], ax=None):\n",
    "    '''Plot two sampled classes on a two-dimensional plane.'''\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    ax.scatter(\n",
    "        X[y==0, 0], X[y==0, 1], color=colors[0], alpha=0.7, edgecolors='none', label='y=0'\n",
    "    )\n",
    "\n",
    "    ax.scatter(\n",
    "        X[y==1, 0], X[y==1, 1], color=colors[1], alpha=0.7, edgecolors='none', label='y=1'\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function(function,\n",
    "                  levels=(0.1, 0.3, 0.5, 0.7, 0.9),\n",
    "                  x_limits=None,\n",
    "                  y_limits=None,\n",
    "                  colorbar=True,\n",
    "                  ax=None):\n",
    "    '''Plot a function of two features on the plane.'''\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    if x_limits is None:\n",
    "        x_limits = ax.get_xlim()\n",
    "\n",
    "    if y_limits is None:\n",
    "        y_limits = ax.get_ylim()\n",
    "\n",
    "    x_values = np.linspace(*x_limits, num=201)\n",
    "    y_values = np.linspace(*y_limits, num=201)\n",
    "\n",
    "    (X_values, Y_values) = np.meshgrid(x_values, y_values)\n",
    "    Z_values = function(np.stack((X_values.ravel(), Y_values.ravel()), axis=1)).reshape(X_values.shape)\n",
    "    \n",
    "    im1 = ax.imshow(Z_values, origin='lower', extent=(*x_limits,*y_limits),\n",
    "                    interpolation='bicubic', cmap='Greys', alpha=0.4) # vmin=0, vmax=1\n",
    "    im2 = ax.contour(X_values, Y_values, Z_values, levels, colors='black', alpha=0.6)\n",
    "\n",
    "    if colorbar:\n",
    "        plt.colorbar(im1)\n",
    "\n",
    "    plt.clabel(im2, fmt='%1.2f')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half-moons data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_samples = 500\n",
    "noise_level = 0.15\n",
    "X, y = make_moons(no_samples, shuffle=True, noise=noise_level)\n",
    "X[y==0,1] += 0.15\n",
    "X[y==1,1] += -0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_data(X, y, ax=ax)\n",
    "ax.set(xlim=(-2, 3), ylim=(-2, 2.5))\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(visible=True, which='both', color='gray', alpha=0.2, linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_scaler = StandardScaler()\n",
    "X_train_normalized = original_scaler.fit_transform(X_train)\n",
    "X_val_normalized = original_scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=5, interaction_only=False, include_bias=False)\n",
    "X_train_poly = polynomial_features.fit_transform(X_train_normalized)\n",
    "X_val_poly = polynomial_features.transform(X_val_normalized)\n",
    "\n",
    "no_features = X_train_poly.shape[1]\n",
    "print('No. features:', no_features)\n",
    "print('\\nFeatures:', polynomial_features.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_scaler = StandardScaler()\n",
    "X_train_final = polynomial_scaler.fit_transform(X_train_poly)\n",
    "X_val_final = polynomial_scaler.transform(X_val_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_final, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val_final, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_set = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,batch_size=len(train_set), shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set, batch_size=len(val_set), shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logistic regression model\n",
    "model1 = nn.Linear(in_features=no_features, out_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=0.1)\n",
    "\n",
    "point_model = Classification(\n",
    "    model1,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "point_history = point_model.training(no_epochs=500, log_interval=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_train_loss, point_train_acc = point_model.test(train_loader)\n",
    "point_val_loss, point_val_acc = point_model.test(val_loader)\n",
    "\n",
    "print('Train loss: {:.4f}'.format(point_train_loss))\n",
    "print('Val. loss: {:.4f}'.format(point_val_loss))\n",
    "print('\\nTrain acc.: {:.4f}'.format(point_train_acc))\n",
    "print('Val. acc.: {:.4f}'.format(point_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.plot(np.array(point_history['train_loss']), label='train', alpha=0.7)\n",
    "ax.plot(np.array(point_history['val_loss']), label='val.', alpha=0.7)\n",
    "ax.set(xlabel='epoch', ylabel='loss')\n",
    "ax.set_xlim((0, point_history['no_epochs']))\n",
    "ax.legend()\n",
    "ax.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variational logistic regression model\n",
    "model2 = VariationalLinear(in_features=no_features, out_features=1, weight_std=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model2.parameters(), lr=0.1)\n",
    "\n",
    "post_model = VariationalClassification(\n",
    "    model2, likelihood_type='Bernoulli'\n",
    ")\n",
    "\n",
    "post_model.compile_for_training(\n",
    "    optimizer, train_loader, val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "post_history = post_model.training(no_epochs=500, no_samples=20, log_interval=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_train_loss = post_model.test_loss(train_loader)\n",
    "post_train_acc = post_model.test_acc(train_loader)\n",
    "post_val_loss = post_model.test_loss(val_loader)\n",
    "post_val_acc = post_model.test_acc(val_loader)\n",
    "\n",
    "print('Train loss: {:.4f}'.format(post_train_loss))\n",
    "print('Val. loss: {:.4f}'.format(post_val_loss))\n",
    "print('\\nTrain acc.: {:.4f}'.format(post_train_acc))\n",
    "print('Val. acc.: {:.4f}'.format(post_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.plot(-np.array(post_history['train_loss']), label='train', alpha=0.7)\n",
    "ax.plot(-np.array(post_history['val_loss']), label='val.', alpha=0.7)\n",
    "ax.set(xlabel='epoch', ylabel='ELBO')\n",
    "ax.set_xlim((0, post_history['no_epochs']))\n",
    "ax.legend()\n",
    "ax.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_features(x):\n",
    "    '''Transform features.'''\n",
    "    x_normalized = original_scaler.transform(x)\n",
    "    x_poly = polynomial_features.transform(x_normalized)\n",
    "    x_final = polynomial_scaler.transform(x_poly)\n",
    "    x_tensor = torch.tensor(x_final, dtype=torch.float32)\n",
    "    return x_tensor\n",
    "\n",
    "def point_prediction(x):\n",
    "    '''Compute normal point predictions.'''\n",
    "    x_tensor = transform_features(x)\n",
    "    point_model.train(False)\n",
    "    with torch.no_grad():\n",
    "        point_logits = point_model.predict(x_tensor.to(point_model.device)).cpu()\n",
    "        point_probs = torch.sigmoid(point_logits)\n",
    "    return point_probs.data.numpy()\n",
    "\n",
    "def posterior_mean(x):\n",
    "    '''Predict with posterior mean weights.'''\n",
    "    x_tensor = transform_features(x)\n",
    "    post_model.sample(False)\n",
    "    post_model.train(False)\n",
    "    with torch.no_grad():\n",
    "        point_logits = post_model.predict(x_tensor.to(post_model.device)).cpu()\n",
    "        point_probs = torch.sigmoid(point_logits)\n",
    "    return point_probs.data.numpy()\n",
    "\n",
    "def posterior_predictive(x, no_samples=1000):\n",
    "    '''Predict according to the posterior predictive distribution.'''\n",
    "    x_tensor = transform_features(x)\n",
    "    post_model.sample(True)\n",
    "    post_model.train(False)\n",
    "    with torch.no_grad():\n",
    "        sampled_logits = post_model.predict(x_tensor.to(post_model.device), no_samples).cpu()\n",
    "        sampled_probs = torch.sigmoid(sampled_logits)\n",
    "    post_mean = torch.mean(sampled_probs, axis=-1)\n",
    "    return post_mean.data.numpy()\n",
    "\n",
    "def posterior_uncertainty(x, no_samples=1000):\n",
    "    '''Compute the uncertainty associated with the posterior predictive.'''\n",
    "    x_tensor = transform_features(x)\n",
    "    post_model.sample(True)\n",
    "    post_model.train(False)\n",
    "    with torch.no_grad():\n",
    "        sampled_logits = post_model.predict(x_tensor.to(post_model.device), no_samples).cpu()\n",
    "        sampled_probs = torch.sigmoid(sampled_logits)\n",
    "    post_std = torch.std(sampled_probs, axis=-1)\n",
    "    return post_std.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_data(X_train, y_train, ax=ax)\n",
    "ax.set(xlim=(-2, 3), ylim=(-2, 2.5))\n",
    "plot_function(point_prediction, levels=(0.3, 0.5, 0.7), ax=ax)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_title('point predictions')\n",
    "ax.legend(loc='upper left')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_data(X_train, y_train, ax=ax)\n",
    "ax.set(xlim=(-2, 3), ylim=(-2, 2.5))\n",
    "plot_function(posterior_mean, levels=(0.3, 0.5, 0.7), ax=ax)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_title('posterior mean')\n",
    "ax.legend(loc='upper left')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_data(X_train, y_train, ax=ax)\n",
    "ax.set(xlim=(-2, 3), ylim=(-2, 2.5))\n",
    "plot_function(posterior_predictive, levels=(0.1, 0.3, 0.5, 0.7, 0.9), ax=ax)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_title('posterior predictive')\n",
    "ax.legend(loc='upper left')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_data(X_train, y_train, ax=ax)\n",
    "ax.set(xlim=(-2, 3), ylim=(-2, 2.5))\n",
    "plot_function(posterior_uncertainty, levels=np.linspace(0.1, 0.9, 9), ax=ax)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_title('posterior uncertainty')\n",
    "ax.legend(loc='upper left')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
